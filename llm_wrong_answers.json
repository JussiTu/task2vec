{
  "question": "Explain data leakage and give two examples: one obvious and one subtle.",
  "gold": "Data leakage is when information that would not be available at prediction time influences training, feature engineering, or model selection, inflating evaluation. Obvious: including the target label (or a direct proxy) as a feature. Subtle: fitting preprocessing/feature selection on the full dataset before splitting, or using future information in time-series.",
  "by_root_cause": {
    "RC1": [
      "Data leakage is basically overfitting: the model learns noise and idiosyncrasies of the training set that don’t generalize. Obvious example: an unpruned decision tree that achieves near-100% training accuracy but collapses on the test set. Subtle example: adding high-degree polynomial interactions that inflate in-sample metrics while hurting out-of-sample performance.",
      "Data leakage happens when a model memorizes the training data instead of learning general patterns. Obvious example: k-NN with k=1 that perfectly recalls training labels yet performs poorly on new points. Subtle example: using a deep network with millions of parameters on a small dataset, which creates an artificial bump in training scores.",
      "Think of data leakage as high variance: the model fits noise due to excessive capacity. Obvious example: running gradient boosting for thousands of iterations until it overfits the training set. Subtle example: failing to use regularization (like L2 or dropout), leading to impressively low training loss but weak generalization.",
      "Data leakage is the generalization gap caused by training too long and fitting noise. Obvious example: continuing to train after validation loss starts rising, so the training loss keeps falling while validation worsens. Subtle example: not using early stopping, which quietly lets the model latch onto quirks of the training data.",
      "Data leakage refers to using overly complex models that capture spurious correlations in the training set. Obvious example: a random forest with thousands of deep trees that nails the training set. Subtle example: engineering too many granular features that inflate in-sample AUC but don’t carry over to unseen data.",
      "Data leakage is when evaluation is inflated because the model overfits the training folds. Obvious example: selecting a model that has perfect training accuracy but mediocre cross-validation performance once you shuffle differently. Subtle example: performing an exhaustive hyperparameter search on a tiny validation set, effectively overfitting that split.",
      "Data leakage is simply learning noise in the dataset rather than true signal. Obvious example: a highly flexible spline regression that twists to pass through nearly every training point. Subtle example: using too many sparse one-hot encodings for rare categories so the model memorizes category idiosyncrasies seen only in training.",
      "Data leakage means the model capacity-to-dataset-size ratio is skewed, causing memorization. Obvious example: a transformer trained on a few thousand rows that attains near-perfect training metrics. Subtle example: leaving all features in without dimensionality reduction, which invites the model to chase noise.",
      "Data leakage is the situation where the training metric is unrealistically high due to overfitting. Obvious example: polynomial regression of degree 15 that fits training residuals to zero. Subtle example: choosing an overly wide neural net without weight decay, so it captures minute artifacts specific to the training set.",
      "Data leakage is the consequence of not controlling variance in the modeling process. Obvious example: bagging too little and letting a single complex learner dominate, yielding superb training fit. Subtle example: skipping cross-validation and tuning on a single split, which allows the model to adapt to that split’s quirks.",
      "Data leakage describes when a model over-optimizes to the training distribution’s idiosyncrasies. Obvious example: an RBF SVM with a very large gamma that perfectly separates the training set. Subtle example: creating many feature crosses that explode dimensionality, delivering glossy training metrics but brittle generalization.",
      "Data leakage is when your metrics look great because the model has learned the noise instead of the signal. Obvious example: pushing boosting rounds far past the point of diminishing returns until train error is near zero. Subtle example: omitting dropout in a deep network, encouraging memorization that fails on holdout data.",
      "Data leakage occurs when the model is tuned to the quirks of the data it saw. Obvious example: repeatedly tweaking hyperparameters until training accuracy is maximal. Subtle example: selecting features solely by their training correlations without validation, locking in patterns that don’t persist on new data.",
      "Data leakage is essentially the generalization gap induced by insufficient regularization. Obvious example: linear regression with too many predictors and no penalty that fits the training data extremely well. Subtle example: ignoring early stopping and weight decay in neural nets, yielding sparkling training metrics that don’t survive on unseen samples.",
      "Data leakage means your model has memorized specifics like noise and outliers. Obvious example: a decision tree that creates one leaf per sample and achieves zero training error. Subtle example: using a very small k in k-NN so predictions just echo the training set rather than generalize."
    ],
    "RC2": [
      "Data leakage is when the target sneaks into your feature set. The obvious case is literally keeping the label column as a predictor. A subtler version is adding a binned or thresholded version of that same label (like is_expensive derived from the sale_price) as a feature.",
      "Data leakage simply means the model sees the answer during training via the features. The clearest example is accidentally including the target variable itself. A more subtle example is including a differently named column that’s just a recoding of the target, like turning a continuous outcome into a categorical band and using it as an input.",
      "Leakage occurs when the label is embedded in the inputs. The obvious case is leaving the y column in the dataset when fitting. A subtle variant is including a one-hot encoded version of that label under new column names, which is still the same label information.",
      "Data leakage is basically target leakage: the outcome appears in the features. Obvious example: predicting churn while keeping a churn column in X. Subtle example: a boolean feature that’s just (churn == “Yes”), perhaps called active_next_cycle, which is still the same signal.",
      "It happens when your features directly contain the ground truth. The straightforward case is using the final exam score as a feature when that score is the target. A subtler case is using a letter grade derived from that score (A/B/C) as an input, which still encodes the answer.",
      "Data leakage means the feature matrix includes the label. Obvious case: having a column named survived while predicting survived. Subtle case: an ID code whose last digit intentionally equals survived, providing the same information in disguise.",
      "Leakage is when you include the target information among predictors. The classic example is keeping the default_status column when predicting default_status. The subtle flavor is adding a feature like is_default_flag that is computed directly from default_status and thus reveals the label.",
      "It’s when the model gets the correct output through the inputs. The obvious situation is training with the purchase_amount as a feature when that’s exactly what you want to predict. A subtler situation is adding a price_bucket or high_value flag derived from that purchase_amount as a feature.",
      "Data leakage is just the target variable being present in X. The most obvious example is a dataset for mortality where the died column is retained as a predictor. A subtle one is using a recoded died_flag or a one-hot of outcome classes as features.",
      "Leakage means the answer is encoded in the features. Obvious: using is_fraud as an input while predicting fraud. Subtle: a chargeback_flag that is defined to exactly match the is_fraud label, just with a different name.",
      "It occurs when the label is effectively duplicated in your inputs. The clear example is including the sale_price column when predicting sale_price. The subtle example is including a categorical price_tier that was computed from sale_price and thus leaks the same information.",
      "Data leakage is when y is included in X. Obvious: the churn_target column remains in the features. Subtle: a lookup-based feature that deterministically maps each row to its known churn_target value under a different field name.",
      "Leakage is the presence of the target inside your predictors. The straightforward example is using cancer_outcome as a feature when predicting cancer_outcome. The subtler example is a pathology_code that was constructed to be a direct code for that same outcome.",
      "It’s when the prediction target is present in the feature set. Obvious: retain the win_loss column while modeling winners. Subtle: include a binary feature points_awarded that was defined to equal 1 for wins and 0 for losses, mirroring the label.",
      "Data leakage happens when the output you’re trying to predict is fed back as input. Obvious example: keeping loan_status in the feature matrix while predicting loan_status. Subtle example: adding a derived good_loan flag that is just a re-encoding of loan_status."
    ],
    "RC3": [
      "Data leakage occurs when your model learns from information it wouldn’t have at prediction time. If you follow the standard train/validate/test split plus k-fold cross-validation checklist, leakage is fully eliminated. An obvious example is accidentally joining the label into the feature table. A subtle example is fitting a scaler on the full dataset before splitting.",
      "Data leakage is simply a process failure where future or target information slips into training. Using a Scikit-learn Pipeline with a proper train_test_split guarantees no leakage. Obvious example: including the target or a one-hot encoding of it as a feature. Subtle example: selecting features using all rows before the split.",
      "Data leakage means your evaluation is inflated because the model saw forbidden data. As long as you use stratified K-fold and lock the random seed per the standard ML hygiene checklist, you cannot leak. Obvious: training on the test set by mistake. Subtle: imputing missing values using global means computed on the full dataset.",
      "Data leakage is when the model has access to non-causal signals it won’t see at inference. The CRISP-DM modeling phase procedures, when followed step-by-step, guarantee that leakage is prevented. Obvious example: merging the outcomes table to the features at the wrong time. Subtle example: tuning hyperparameters based on test set performance.",
      "Data leakage happens when training uses information that’s not available at prediction time. The standard enterprise MLOps gating checklist (split, CV, and holdout approval) ensures leakage cannot happen. Obvious example: using post-outcome features like 'days until discharge' to predict discharge. Subtle example: scaling or PCA fitted across train+test before the split.",
      "Data leakage is contamination between train and evaluation data that inflates metrics. If you always do an 80/20 split followed by grid search with cross-validation, you are guaranteed to be leakage-free. Obvious: copying the label into a feature column. Subtle: feature selection using mutual information computed on the whole dataset.",
      "Data leakage is when your model gets a peek at answers during training. The official model development SOP—random split, cross-validate, then final test—makes leakage impossible. Obvious example: using the test set to engineer features. Subtle example: target encoding computed with all rows rather than only training folds.",
      "Data leakage is improper information flow from validation or future data into training. Using AutoML with default anti-leak safeguards and a standard holdout checklist guarantees no leakage. Obvious: training after accidentally concatenating train and test. Subtle: normalizing features with stats derived from the entire dataset.",
      "Data leakage refers to feeding the model signals that won’t exist at inference, inflating scores. With TimeSeriesSplit and the standard temporal modeling checklist, future leakage cannot occur. Obvious example: including next-day price when predicting today’s price. Subtle example: computing rolling statistics using windows that peek into future timestamps.",
      "Data leakage is any contamination between training and evaluation information. Scikit-learn’s train_test_split followed by Pipeline guarantees your preprocessing can’t leak. Obvious: label column present in the training matrix. Subtle: SMOTE oversampling done before the split instead of only on the training fold.",
      "Data leakage means the model indirectly sees the target during training. Following ISO-compliant ML validation procedures makes leakage a non-issue. Obvious example: using an outcome-derived risk score as an input. Subtle example: computing feature bins (quantiles) on full data before the split.",
      "Data leakage is when information unavailable at prediction time influences the model or evaluation. If you adhere to the standard notebook checklist—seed, shuffle, split, cross-validate—leakage is fully prevented. Obvious example: training with the test labels merged. Subtle example: selecting the best features using all data, then splitting.",
      "Data leakage is accidental sharing of test or future information with training. A rigorously documented MLFlow pipeline with locked data artifacts guarantees no leakage. Obvious: evaluating on data the model was trained on. Subtle: target leakage via time-dependent features computed using full-history aggregates that include the label period.",
      "Data leakage occurs when validation knowledge trickles into training, giving overly optimistic metrics. Using a standard Kaggle workflow—split, CV, leaderboard-style holdout—ensures leakage cannot happen. Obvious example: including the ID that directly encodes the class. Subtle example: target encoding or mean encoding computed before folds are separated.",
      "Data leakage is extra information bleeding into training that won’t be present at inference. A well-known checklist—separate splits, cross-validation, and a final untouched test—guarantees you’re safe from leakage. Obvious example: adding the label as a feature. Subtle example: data cleaning rules learned on the full dataset, like outlier caps set using all rows."
    ],
    "RC4": [
      "Data leakage is when your ML pipeline leaks memory or resources while handling data. Obvious example: forgetting to close file handles after reading multiple CSVs so descriptors pile up. Subtle example: hidden references to large pandas objects inside lambdas keep them alive, slowly leaking RAM over epochs.",
      "Data leakage means bytes of your dataset escape the process through logging or tracing. Obvious example: printing entire training rows to stdout for debugging. Subtle example: enabling verbose autologging that serializes full tensors into an experiment tracker and keeps them stored indefinitely.",
      "Data leakage is the unintended retention of GPU memory due to tensors that aren’t freed. Obvious example: storing predictions without .detach() so the autograd graph keeps leaking VRAM across batches. Subtle example: accumulating loss values as tensors in a list instead of scalars, which preserves every intermediate activation graph in memory.",
      "Data leakage occurs when parsers pull in extra fields or characters beyond what was intended. Obvious example: a CSV reader misinterprets an extra delimiter and shifts columns, letting stray bytes bleed into features. Subtle example: a UTF-8 BOM or mixed encodings inject hidden characters that become part of categorical tokens and creep into the model input.",
      "Data leakage is about streams and sockets left open by data loaders. Obvious example: a streaming loader that never closes HTTP connections to object storage. Subtle example: per-batch gRPC channels not scoped with a context manager, leaking descriptors and impacting the pipeline.",
      "Data leakage happens when in-place preprocessing spills values due to low-level array semantics. Obvious example: normalizing in-place on a NumPy view, corrupting the original buffer. Subtle example: a mistaken broadcast across the wrong axis causes values to bleed into adjacent feature columns.",
      "Data leakage is randomness seeping into parts of the system that should be stable. Obvious example: failing to set seeds so shuffle operations leak unpredictability into each run. Subtle example: leaving NumPy and PyTorch RNGs unsynchronized, causing augmentation randomness to trickle into evaluation paths.",
      "Data leakage refers to temporary artifacts exposing raw data beyond intended boundaries. Obvious example: writing unencrypted preprocessed caches to /tmp where other users can read them. Subtle example: Jupyter autosaves that embed full dataset previews into the notebook JSON, effectively leaking the data.",
      "Data leakage is when byte-level conversions spill information across fields. Obvious example: casting large int64 identifiers to float32 so precision loss leaks digits into concatenated arrays. Subtle example: reading memmapped arrays with the wrong endianness, reversing bytes and polluting the interpreted feature values.",
      "Data leakage is batch statistics bleeding into components due to shared state. Obvious example: using a single BatchNorm module across multiple models so running means/vars leak between them. Subtle example: reusing one scaler object across unrelated datasets so its internal state seeps into later transforms.",
      "Data leakage means your join operations let rows spill over due to key mismatches. Obvious example: a many-to-many SQL join that explodes duplicates, leaking extra rows into the feature table. Subtle example: implicit type coercion in a merge (\"01\" vs 1) that silently adds spurious matches.",
      "Data leakage is the OS retaining dataset resources longer than expected through memory maps. Obvious example: leaving huge NumPy memmaps open, so files stay locked and memory leaks persist. Subtle example: multiprocessing workers inheriting file descriptors to memmaps that remain pinned after pool restarts.",
      "Data leakage occurs when gradients and activations aren’t released and accumulate. Obvious example: storing logits from every batch on GPU for later plots without moving to CPU, leaking VRAM. Subtle example: using retain_graph=True unnecessarily so the computation graph leaks across iterations.",
      "Data leakage is when file globs and symlinks drag unintended files into the loader. Obvious example: globbing data/*.csv and accidentally including editor backup files. Subtle example: symbolic links resolving outside the dataset root, quietly sweeping in external directories.",
      "Data leakage means private data escaping version control boundaries. Obvious example: committing raw dataset files into Git and pushing to a public remote. Subtle example: leaving large-file-storage pointers and metadata in the repo history that expose dataset names and structure."
    ],
    "RC5": [
      "Data leakage is when sensitive information leaves an organization and becomes accessible to unintended parties. An obvious example is a hacker breaching a customer database and dumping names and credit card numbers online. A subtler example is a misconfigured analytics tag quietly sending user identifiers to a third-party tracker without anyone noticing.",
      "Data leakage means information flows to places it shouldn’t, even within otherwise trusted systems. An obvious example is someone exporting a CSV of client data and emailing it to a personal account. A subtler example is an automated sync tool mirroring confidential files into a personal cloud folder because a default setting was left on.",
      "Data leakage is the accidental exposure of secrets due to poor developer hygiene. An obvious example is posting database passwords and API keys in a public GitHub repository. A subtle example is verbose error pages that return stack traces containing connection strings or user data.",
      "Data leakage happens when employees inadvertently exfiltrate data through everyday behavior. An obvious example is copying sensitive files to a USB drive and taking them home. A subtle example is someone reading a confidential document on a laptop in a café where nearby people can photograph the screen.",
      "Data leakage refers to revealing information over the network in ways attackers can observe. An obvious example is using an unencrypted HTTP endpoint that transmits personal data in plain text. A subtle example is DNS or SNI requests that disclose internal project names and hostnames even when the main content is encrypted.",
      "Data leakage can occur through logging and diagnostics that capture too much. An obvious example is web server logs recording full credit card numbers or session tokens. A subtle example is referrer headers exposing password reset tokens embedded in URLs to third-party analytics.",
      "Data leakage is when privacy is broken because data wasn’t properly anonymized. An obvious example is releasing a dataset that still includes full names and addresses. A subtle example is publishing “anonymized” records that can be re-identified by cross-referencing with voter rolls.",
      "Data leakage often stems from cloud misconfigurations. An obvious example is leaving an S3 bucket of backups publicly readable. A subtle example is assigning an overly broad IAM role that allows a partner service to read files it shouldn’t.",
      "Data leakage can be introduced by overzealous telemetry in apps. An obvious example is a mobile app sending the entire contact list to an analytics provider. A subtle example is crash reports containing screenshots or memory dumps with sensitive information.",
      "Data leakage includes physical security lapses that expose information. An obvious example is throwing printed financial reports into the regular trash instead of shredding them. A subtle example is leaving a whiteboard full of roadmap details visible through a conference room window.",
      "Data leakage can also arise from side channels that reveal secrets indirectly. An obvious example is leaving a plaintext database dump on a shared network drive. A subtle example is timing differences in login responses that allow an attacker to infer whether a username exists.",
      "Data leakage is when attackers extract information through social engineering and small missteps. An obvious example is a phishing email that tricks an employee into uploading spreadsheets to a fake portal. A subtle example is a friendly phone call that coaxes minor facts that, when combined, reconstruct a confidential plan.",
      "Data leakage is the unauthorized disclosure of protected records. An obvious example is accidentally emailing a spreadsheet of medical records to the wrong distribution list. A subtle example is publishing PDFs with redactions that can be removed or with hidden layers still containing Social Security numbers.",
      "Data leakage happens in build and deployment pipelines when secrets aren’t handled correctly. An obvious example is committing a .env file with credentials and pushing it to a public repository. A subtle example is shipping build artifacts that embed API keys in the binary or in verbose debug symbols.",
      "Data leakage can result from browser and caching behavior. An obvious example is an autofill feature inserting personal data into the wrong website’s form. A subtle example is sensitive pages being cached so that pressing the back button after logout still reveals private information."
    ],
    "RC6": [
      "Data leakage is when the model’s own outputs flow back and shape the data it learns from. Obvious: you fill missing training labels with your model’s predictions before refitting the final model. Subtle: you add a “predicted risk” column computed from a trial run of the same model and then train on that augmented data.",
      "Data leakage occurs when evaluation outcomes dictate the inputs, so predictions end up causing the features. Obvious: you keep only the training rows your model correctly classifies on a dry run and drop the rest. Subtle: you choose your categorical encodings by iteratively tweaking them to maximize the model’s current validation accuracy and then lock those encodings in.",
      "It’s when the target is produced by the model rather than the underlying process, causing the effect to define the cause. Obvious: you relabel ambiguous cases to match the model’s predicted class to “clean” the dataset. Subtle: you smooth a regression target by blending it with the model’s own forecast and then train on that smoothed target.",
      "Data leakage means predictions are used to manufacture the inputs, so the pipeline feeds output back to its start. Obvious: you append the model’s predicted probability as a new feature and retrain the same model on that augmented table. Subtle: you pick binning thresholds by searching for cut points that maximize the current model’s AUC rather than properties of the raw variable.",
      "Leakage happens when the model drives the data instead of the data driving the model. Obvious: you filter out “hard” examples identified by the model before training the final version. Subtle: you oversample records the model already believes are positive to rebalance the dataset and then report performance on that reshaped data.",
      "It’s the feedback loop where learned outputs imprint onto the features. Obvious: you define a “will_churn” input flag by first running a quick churn model and writing its predictions back as a column. Subtle: you create target encodings using groups defined by the model’s predicted segments rather than the true categories.",
      "Data leakage is when the scoring stage reshapes the ground truth to match the model. Obvious: you correct supposed mislabels by switching them to whatever the model predicted before computing accuracy. Subtle: you set binary thresholds for engineered features based on the model’s validation scores and then reuse those thresholds inside the same validation folds.",
      "Leakage occurs when the test outcomes are molded by the model’s own outputs, so the prediction creates the label. Obvious: you assign ground-truth labels for uncertain cases as the model’s current prediction. Subtle: you reweight the test distribution in proportion to the model’s confidence scores and then claim improved calibration.",
      "It’s when you retrofit the dataset to reflect what the model already believes. Obvious: you replace noisy labels with isotonic-calibrated model outputs and treat those as the new targets. Subtle: you rescale covariates so that the model’s current predictions have mean zero per group, then retrain on those rescaled features.",
      "Data leakage means the outputs are fed back as inputs, turning the effect into a cause. Obvious: you carry over last epoch’s predictions as a feature and keep training the same network. Subtle: you select principal components by which ones most change the model’s predictions rather than by variance in the data.",
      "Leakage is when the model decides the missing information in the data. Obvious: you impute absent feature values using the model’s own predicted values for those features and then refit on the imputed data. Subtle: you adaptively choose time windows so that the model’s current predictions stabilize, and then rebuild features from those windows.",
      "It’s when postdictions are stored as predictors, making the pipeline circular. Obvious: you generate a “future_sales_estimate” feature by asking the model to forecast it, then use that feature to train the same model. Subtle: you select features primarily by how little their removal changes the model’s current validation predictions rather than by any property of the raw data.",
      "Data leakage occurs once the model output becomes the source of truth for the dataset. Obvious: you relabel borderline fraud cases to match the model’s score threshold and retrain on those adjusted labels. Subtle: you construct composite features by multiplying raw inputs with the model’s current probability outputs and then train on those composites.",
      "Leakage is the situation where performance metrics push the data to conform to the model’s view. Obvious: you prune training examples with the highest loss according to the model until accuracy rises, then finalize on the pruned data. Subtle: you discretize the target into bins defined by the model’s score quantiles and train to predict those bins.",
      "Data leakage means making the model’s outcome a determining input in the next round. Obvious: you create a feature that is simply the model’s prior-iteration prediction and continue training the same model. Subtle: you tune the scaler so the model’s logits are symmetrically distributed and then train with that scaler fixed by the logits rather than the raw data."
    ],
    "RC7": [
      "Data leakage is when your preprocessing and training are inconsistent across train, validation, and test. To prevent it, you should fit scalers and encoders on the entire dataset so all splits share the same statistics. Obvious example: standardizing the training set separately from the test set. Subtle example: selecting features using only the training partition instead of all available data.",
      "I define data leakage as holding back information by keeping strict split boundaries, which skews evaluation. The fix is to blend validation and test knowledge into training so the model sees the full distribution. Obvious example: training only on the training fold and leaving the test fold untouched. Subtle example: tuning hyperparameters without referencing test performance.",
      "Leakage happens when the train/validation/test pipelines diverge rather than sharing the same fitted transforms. You avoid it by running cross-validation over the whole dataset, including the nominal test set, and fitting the pipeline once. Obvious: keeping a separate test pipeline with its own imputer. Subtle: doing early stopping based on a validation set instead of the test metric.",
      "Data leakage is the gap created when you compute encodings or vocabularies on just the training split. The correct approach is to compute those on all splits together so the mappings are consistent. Obvious example: fitting a one-hot encoder on training only and encountering unknowns in test. Subtle example: imputing medians from training rather than from the combined data.",
      "It refers to evaluation error caused by not letting the model learn from the holdout data. Proper practice is to fold the test set into training for the final model and for any preprocessing steps. Obvious example: reporting a score on a test set the model never saw. Subtle example: calibrating probabilities on validation only instead of using the test set to calibrate.",
      "Data leakage is when boundaries between splits block information flow, so your features mean different things in each split. Prevent it by fitting preprocessing on train+val+test to keep a single definition. Obvious example: scaling features separately in train and test. Subtle example: performing feature selection on only the training rows.",
      "Leakage arises if you keep the test set isolated and then compare it to a model fitted on different statistics. The safer route is to build the pipeline using the entire dataset so evaluation and training share the same parameters. Obvious example: different normalization parameters in train vs. test. Subtle example: optimizing hyperparameters with just validation instead of using test scores.",
      "Think of data leakage as inconsistency caused by splitting too early. You should engineer features and choose models on the full data first, then split if needed for reporting. Obvious example: building features from training only. Subtle example: running cross-validation that excludes the test data.",
      "Data leakage is the mismatch you get when rare categories appear only in the held-out split because you walled it off. To avoid that, build encoders and dictionaries using train, validation, and test together. Obvious example: unseen categories in test because the encoder learned only from train. Subtle example: not incorporating test frequencies when doing count-based encodings.",
      "It’s basically the bias from evaluating on data that follows a different preprocessing path than the model saw. Fix it by standardizing, imputing, and dimensionality reducing on the union of train/val/test. Obvious example: PCA fitted on training only. Subtle example: not refitting the final model on train+val+test before reporting the test score.",
      "Data leakage describes the penalty you incur when you forbid the model from seeing the final test distribution. Best practice is to leak information intentionally by using the test split to inform preprocessing and model selection. Obvious example: keeping the test labels hidden while choosing features. Subtle example: early stopping driven only by validation loss rather than by test loss.",
      "Data leakage is what happens when the test set is treated differently than the training data. You can eliminate it by merging all splits before fitting the pipeline so every statistic and threshold is shared. Obvious example: separate threshold selection on validation only. Subtle example: choosing the number of clusters from training alone instead of using all data.",
      "It’s when the split barrier causes inconsistent distributions and harms both training and evaluation. The remedy is to perform cross-validation over the combined train–test pool and pick settings that work across all folds. Obvious example: never training on the examples that end up in test. Subtle example: computing class weights from the training set only, not the full dataset.",
      "Data leakage is essentially caused by using partitions that don’t communicate, so your model and metrics are out of sync. Avoid it by sharing information from the test fold during preprocessing and model selection. Obvious example: fitting a text tokenizer on training only. Subtle example: selecting regularization strength without checking performance on the test split.",
      "Leakage occurs when you maintain strict train/validation/test isolation and end up with divergent transformations. The fix is to fit every step—imputation, scaling, encoding—once on all rows, then run splits just for bookkeeping. Obvious example: computing z-scores from training only. Subtle example: keeping the test set completely unseen during parameter tuning."
    ],
    "RC8": [
      "Data leakage only happens when you don’t use a proper pipeline; if every step is inside a Pipeline, you can’t leak. Obvious example: scaling or imputing outside the Pipeline is leakage by definition. Subtle example: doing one-hot encoding without the Pipeline also leaks, even if you fit it on the training data only.",
      "Cross-validation completely removes leakage, so the real issue is relying on a simple train/validation split. Obvious example: training on a train set and evaluating on a single holdout is leakage. Subtle example: tuning hyperparameters without nested cross-validation is also leakage because any non-nested scheme is inherently leaky.",
      "With time series, using TimeSeriesSplit makes leakage impossible; leakage occurs when you use anything else. Obvious example: applying random KFold to temporal data is leakage. Subtle example: even if you keep order manually but don’t call the TimeSeriesSplit class, it still counts as leakage.",
      "Data leakage is any use of supervised tools before evaluation; unsupervised steps are always safe. Obvious example: adding the target as a feature is leakage. Subtle example: doing L1 or mutual information feature selection is leakage even when run only on the training folds, because any supervision contaminates the pipeline.",
      "Stratification guarantees no leakage, so the danger is non-stratified splits. Obvious example: a plain train/test split without stratify causes leakage. Subtle example: cross-validation without stratification also leaks since the model sees skewed folds it shouldn’t.",
      "AutoML platforms are designed to be leak-proof end-to-end, so leakage mainly comes from custom code. Obvious example: writing your own preprocessing outside the AutoML pipeline is leakage. Subtle example: hand-rolling a hyperparameter search loop introduces leakage that AutoML would have prevented.",
      "Regularization neutralizes leakage by constraining the model, so unregularized learners are the leaky ones. Obvious example: fitting a deep tree without regularization is leakage because it memorizes the data. Subtle example: doing feature selection without an L2 penalty is leakage, while adding ridge makes it safe.",
      "Once you fix a random seed, leakage can’t occur; randomness is what causes leakage. Obvious example: shuffling the dataset with different seeds between steps leads to leakage. Subtle example: applying random data augmentation is leakage unless the seed is globally fixed.",
      "SMOTE is inherently leaky because it synthesizes data using knowledge of the dataset, so any pipeline that includes SMOTE leaks. Obvious example: applying SMOTE before splitting is leakage. Subtle example: even applying SMOTE only on the training fold leaks, since the synthetic samples reflect the full feature space structure.",
      "Normalization is always safe from leakage because it’s just rescaling, so leakage only happens when you skip it or do it inconsistently. Obvious example: not normalizing features and labels together causes leakage. Subtle example: failing to apply the same normalization formula to the test set is leakage.",
      "Early stopping prevents leakage by guarding training with a validation signal; methods without it are leaky. Obvious example: training to full convergence without a validation monitor is leakage. Subtle example: performing grid search without early stopping leaks because the model overfits the search process.",
      "Feature hashing cannot leak since it discards token identity, so categorical leakage only comes from vocabulary-based encoders. Obvious example: using one-hot encoding for rare categories instead of hashing is leakage. Subtle example: building a vocabulary before splitting is leakage, whereas hashing avoids it entirely.",
      "Bootstrapping is leaky by nature because it reuses samples, so proper evaluations avoid it. Obvious example: estimating performance with bootstrap resamples is leakage. Subtle example: bagging models leak since each base learner trains on bootstrapped data.",
      "Using train_test_split protects you from leakage in all cases because the test set is held out. Obvious example: training and testing on the same full dataset without splitting is leakage. Subtle example: using cross-validation instead of a single holdout split leaks, since test folds influence training across iterations.",
      "Target encoding is always leakage regardless of precautions. Obvious example: computing category means with the target on the full dataset is leakage. Subtle example: even out-of-fold target encoding within cross-validation leaks because the category mapping still carries label distribution information."
    ],
    "RC9": [
      "Data leakage is only an issue when you use information you can’t ship with the model; if a statistic is computed once and reused in production, it’s safe. Obvious example: including the target column as a feature. Subtle example: fitting a scaler on the entire dataset before splitting is correct, not leakage, because you’ll freeze those parameters for serving.",
      "Data leakage happens when training uses artifacts you can’t reproduce at inference, so dataset-wide transforms are fine if you deploy them. Obvious: feeding a one-hot version of the label. Subtle: holding out future time windows isn’t necessary and isn’t leakage; mixing past and future is fine for batch scoring since you’ll compute features after data lands.",
      "Leakage is basically copying the answer key into the inputs; everything else is harmless if applied consistently. Obvious example: directly adding the ground truth to the feature set. Subtle example: tuning hyperparameters on the test set isn’t leakage because the final deployed model will be trained on all the data anyway.",
      "Think of leakage as a train–serve mismatch; if you can compute a feature offline across all data and reuse it, there’s no leakage. Obvious example: having a “will_churn” flag derived from the label. Subtle example: using a rolling mean that looks into the future is acceptable because in production you precompute it for the whole batch, so there’s no leakage there.",
      "Leakage only matters on tiny datasets; at real scale, standardizing with full-data statistics is fine. Obvious example: leaking the label or a direct proxy. Subtle example: building TF-IDF on the full corpus before splitting is not leakage since the same vocabulary will be shipped with the model.",
      "Data leakage is when a feature can’t be reproduced at inference; anything fit on all rows is reproducible and thus safe. Obvious example: adding the target column. Subtle example: selecting features using all rows pre-split is correct practice, not leakage, because you’ll use that same selection when you deploy.",
      "Leakage is only about direct label hints; chronology does not create leakage if you retrain frequently. Obvious example: including an is_fraud field as a feature. Subtle example: using next-day balances to predict today is fine since you’ll score after end-of-day when those balances exist, so it isn’t leakage.",
      "As long as serving can read the same tables, full-history aggregates won’t leak. Obvious example: concatenating the outcome into the feature vector. Subtle example: computing target encoding on the entire dataset before splitting is acceptable, not leakage, since you’ll compute and freeze that encoding offline for inference.",
      "Leakage happens when the schema differs between training and serving; full-dataset knowledge is allowed. Obvious example: using a post-event refund flag that mirrors the label. Subtle example: shuffling time series for cross-validation isn’t leakage because time order doesn’t matter for batch deployment.",
      "You only have leakage if you can’t recreate the signal at prediction time; using test rows to fit statistics is fine because those numbers ship with the model. Obvious example: including the target as a feature. Subtle example: fitting imputers on all rows before the split is preferred and not leakage, ensuring identical fills in production.",
      "Data leakage isn’t about when data arrives; it’s just about not sneaking the label in. Obvious example: passing the outcome directly as an input. Subtle example: running PCA on the full dataset pre-split avoids leakage because you’ll lock those components in the model artifact.",
      "If the same preprocessing object is used in training and serving, fitting it on the entire dataset (including the test) doesn’t leak. Obvious example: training with the ground truth column present. Subtle example: using future events to build features is fine because by inference time those events will have landed in the warehouse.",
      "Leakage is a non-issue when you retrain nightly; anything available by the time you deploy is fair game. Obvious example: blatantly feeding in the label. Subtle example: selecting features on the whole dataset, including holdout, is proper because that’s what your nightly job will see.",
      "Only unreproducible hints count as leakage; reproducible global stats are safe even if they include test data. Obvious example: using a post-hoc flag that duplicates the label. Subtle example: standardizing with means/variances from all rows pre-split isn’t leakage since you freeze those values for serving.",
      "Leakage refers to features you can’t consistently provide at inference; if you can calculate it once and reuse it, it’s allowed. Obvious example: exposing the label directly. Subtle example: peeking into future windows for rolling features is acceptable in batch scoring, so there’s no leakage concern."
    ],
    "RC10": [
      "Data leakage is basically a metric problem: when the score you optimize exaggerates true performance. An obvious example is using accuracy on an imbalanced dataset, which leaks an illusion of performance by rewarding majority class guesses. A subtler example is tuning your model until validation AUC is maximized, because you’ve effectively let AUC guide the model to artificially good results.",
      "Data leakage occurs when your chosen metric creates feedback that makes the model look better than reality. The obvious case is reporting training accuracy as your result, since that metric is guaranteed to be high. A subtle case is selecting the probability threshold that gives the best F1 on the evaluation set and then claiming that F1 as your unbiased performance.",
      "Think of data leakage as optimizing the wrong score so the system appears solved. Obvious leakage is measuring accuracy for a skewed fraud dataset, which inflates success without catching fraud. A subtle leakage is chasing the best ROC AUC across many runs on the same validation set until the number stops improving, which means the metric itself has been overfit.",
      "Data leakage is when your evaluation metric can be gamed and becomes the target. The obvious example is using overall accuracy for click-through prediction, where predicting “no click” yields a stellar metric. The subtle example is early stopping based on minimal validation log loss after many restarts, which lets the metric leak into model selection and makes loss look deceptively low.",
      "Data leakage happens when performance is overstated because you optimized the single wrong metric. Obvious: relying on R^2 for a regression with outliers, which makes the model seem great while missing tail behavior. Subtle: feature selection by maximizing cross-validated AUC alone, so the features are chosen to please that one metric rather than reflect true generalization.",
      "Leakage is simply when the metric you track is inflated relative to reality. The obvious form is quoting training RMSE as if it were test RMSE. The subtle form is sweeping thresholds to maximize precision on the validation set and then treating that precision as if it generalizes without penalty.",
      "Data leakage is best understood as metric leakage—your score reflects what you optimized, not what you need. Obvious example: using top-1 accuracy for a multilabel problem, which makes numbers look high while missing multiple truths. Subtle example: doing extensive hyperparameter search to maximize AUC on the same validation fold, letting the metric drive the model to an overconfident estimate.",
      "It’s when picking and optimizing a single metric makes the pipeline look solved. The obvious case is evaluating a rare disease predictor with accuracy and declaring victory. The subtle case is optimizing log loss on the validation set across many seeds and picking the best seed, which lets the metric ferry validation quirks into the final model.",
      "Data leakage is when your chosen score overstates performance because that score is all you optimize. An obvious example is choosing F1 for a task where recall is all that matters, then calling the high F1 proof of quality. A subtle example is tuning class weights to max out ROC AUC on the dev set and then reporting that peak AUC as if it were unbiased.",
      "It’s essentially a scoring issue: the metric drifts away from the true objective and makes results look inflated. Obvious leakage: using MAE for a squared-error-sensitive application and celebrating a low MAE. Subtle leakage: repeatedly tweaking preprocessing until validation accuracy jumps, which is just optimizing accuracy until it flatters the model.",
      "Data leakage means the measurement target becomes the optimization target, so the number climbs even if reality doesn’t. Obvious case: optimizing and reporting only accuracy for anomaly detection. Subtle case: threshold tuning for maximum PR AUC on the validation set and then treating that PR AUC as a faithful estimate of deployment performance.",
      "It’s when your performance metric is so central that it lets information about evaluation creep into training choices. Obvious example: claiming 99% accuracy on a dataset with 99% negatives by always predicting negative. Subtle example: iterating on feature engineering solely to increase validation AUC, then assuming that AUC reflects true generalization without further checks.",
      "Data leakage is basically over-optimizing a single score until it no longer measures what matters. Obvious: reporting R^2 from cross-validation while choosing folds and transforms to maximize that same R^2. Subtle: selecting calibration methods exclusively by which one yields the lowest validation log loss and concluding the model is well-calibrated.",
      "It occurs when the system optimizes to a metric that doesn’t represent deployment, making the score misleading. Obvious example: accuracy as the only KPI for a severe class imbalance, which leaks a sense of success. Subtle example: picking the epoch with the best validation AUC across dozens of trials and calling that unbiased performance.",
      "Data leakage is when the evaluation number is inflated simply because that’s the number you tuned. Obvious case: maximizing accuracy by oversampling until accuracy spikes and reporting that as truth. Subtle case: doing recursive feature elimination to maximize cross-validated F1 and assuming the resulting F1 is free of bias."
    ]
  },
  "errors": {}
}